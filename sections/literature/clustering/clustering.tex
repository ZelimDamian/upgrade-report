\section{Clustering of Related Objects}\label{lit-clustering}

A rich potential source of information with relation to structural components or other artefacts within software is their similarity (or dissimilarity) in terms of a number of different data sources. Data sources can include reverse engineering (class structural relationships) or source code repositories (co-commit semantics) as used by \citep{beyer_clustering_2005}. Once initial data has been generated from a given source not only is it possible to determine specific similarities but to cluster components into groups which can be used for analysis, further grouping (sub-clusters) or comparison between data sources.

\subsection{Generation of a Normalised Dissimilarity Matrix}

To facilitate comparison between different data sources showing relationships between components (multi-variate analysis) it is necessary to convert the data to a standard form, a normalised dissimilarity matrix \citep{rogers2011first}. The steps followed from \citet{rogers2011first} and \citet{hongbo2010data} to achieve this are as follows.

The first step is to identify commonalities between components and generate a matrix similar to that shown in table \ref{tab-matrix-similarity} for commonalities between components \textit{A}, \textit{B}, and \textit{C} for data source (variable) \textit{X}.

\begin{table}[H]
\centering
\begin{tabular}{| c || c | c | c |}
\hline
 & A & B & C \\ \hline \hline
A & - & - & - \\
B & 2 & - & - \\ 
C & 3 & 4 & - \\ 
\hline
\end{tabular}
\caption{Similarity or Commonalities of Components \textit{A}, \textit{B}, \textit{C} for a given data source \textit{X}}
\label{tab-matrix-similarity}
\end{table}

A similarity matrix can be converted to a distance matrix by subtracting each similarity value from the maximum similarity.

\begin{equation}
\label{eq-matrix-distance}
\displaystyle X' = \sum\limits_{i=1}^n X_{max} - X_i
\end{equation}

In the case of the example data shown in table \ref{tab-matrix-similarity} the resultant distance matrix would be as shown in table \ref{tab-matrix-distance}.

\begin{table}[H]
\centering
\begin{tabular}{| c || c | c | c |}
\hline
 & A & B & C \\ \hline \hline
A & - & - & - \\
B & 2 & - & - \\ 
C & 1 & 0 & - \\ 
\hline
\end{tabular}
\caption{Distance Matrix of Components \textit{A}, \textit{B}, \textit{C} for a given data source \textit{X}}
\label{tab-matrix-distance}
\end{table}

More commonly used is a dissimilarity matrix where the dissimilarity is the square root of the distance as calculated by equation \ref{eq-matrix-dissimilarity}, with example results in table \ref{tab-matrix-dissimilarity}.

\begin{equation}
\label{eq-matrix-dissimilarity}
\displaystyle X' = \sum\limits_{i=1}^n \sqrt{X_{max} - X_i}
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{| c || c | c | c |}
\hline
 & A & B & C \\ \hline \hline
A & - & - & - \\
B & 1.414 & - & - \\ 
C & 1 & 0 & - \\ 
\hline
\end{tabular}
\caption{Dissimilarity Matrix for Components \textit{A}, \textit{B}, \textit{C} for a given data source \textit{X}}
\label{tab-matrix-dissimilarity}
\end{table}

To normalise ranges, ensuring consistent impact when using multi-variate input (multiple data sources for comparison where sources have different absolute numbers of similarities), it is also required to further subtract the mean and divide by the standard deviation, as shown in equation \ref{eq-matrix-normdissimilarity} with example output in table \ref{tab-matrix-normdissimilarity}.

\begin{equation}
\label{eq-matrix-normdissimilarity}
\displaystyle X'' = \sum\limits_{i=1}^n \frac{X'_i - \bar{X'}}{X'_{sd}}
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{| c || c | c | c |}
\hline
 & A & B & C \\ \hline \hline
A & - & - & - \\
B & 0.838 & - & - \\ 
C & 0.269 & -1.107 & - \\ 
\hline
\end{tabular}
\caption{Normalised Dissimilarity Matrix for components \textit{A}, \textit{B}, \textit{C} for a given data source \textit{X}}
\label{tab-matrix-normdissimilarity}
\end{table}

\subsection{Multi-Dimensional Scaling (MDS)}

Multi-Dimensional scaling is a technique used to add additional dimensions to variate data to facilitate more accurate representation within a spatial model for analysis or visualisation \citep{pilch_diss_pich_2009}.

For example consider the distance data represented in table \ref{tab-matrix-distance}. The distances between components are as follows: $A \Rightarrow B = 2$, $A \Rightarrow C = 1$ and $B \Rightarrow C = 0$. In a singular dimension these distances cannot be represented as $B$ and $C$ must be together ($B \Rightarrow C = 0$) but at the same time $B$ and $C$ are different distances from $A$. To resolve this some alteration of the data would be required perhaps averaging distances between points so that $A \Rightarrow C > 1$ and $B \Rightarrow C > 0$.

To alleviate this problem additional dimensions could be added. If the above example was scaled to two dimensions although some alteration would still be required this would be less than on a single dimension. The position of the points in two-dimensional space would be calculated where the Euclidean distance (or similar distance measure) was closest to the absolute distance between the data points.

\begin{equation}
\label{eq-mds-euclid}
\displaystyle A \Rightarrow B \approx \sqrt{(A_x - B_x)^2 + (A_y - B_y)^2}
\end{equation}

\subsection{Clustering Techniques}

There are a significant number of highly varied techniques to approach clustering of related data items including randomised initialisation of a set number of clusters to highly complex models \citep{hongbo2010data,rogers2011first}. Only limited literature for two methods has been formally reviewed thus far given the clustering work on this project is at an early stage.

\subsubsection{K-Means Clustering}

K-Means clustering is a process by which data is partitioned into K partitions where each partition is regarded as a cluster. Each data point is within one partition and hence one cluster. The basic process is iterative by nature and begins by randomly selecting K points as centres (centroids) of prototype or prospective partitions. Data points are then assigned to the nearest centroid usually based on simple Euclidean distance. Centroids are then relocated to the mean centre of their respective data points and the process is repeated with data points reassigned to their nearest centroid. Once the assignments stabilise (no further changes to data point assignment occur), the partitions are finalised \citep{hongbo2010data}.

Because of the randomised nature of the initial points the partitioning can be very imprecise and although iterations will naturally lead to partitions becoming more refined the output can be highly varied and more suited to certain types and distributions of data \citep{hongbo2010data,rogers2011first}.

Variations on K-Means exist including K-Mode (the mode rather than the mean of partitioned data is used for positioning of the centroid) and K-medoid (where actual data points are used as centroids e.g. the data point nearest to the mean centre of a partition is used) \citep{hongbo2010data}. Methods are also in use to improve both the refinement and consistency of output by using informed as opposed to random starting positions for centroids \citep{rogers2011first}, something which is implemented in Weka models \citep{hongbo2010data}.

\subsubsection{Expectation-Maximisation Clustering}

Expectation-Maximisation (EM) clustering is an approach that uses statistical probability calculations to try and find then apply the best model to fit the presented data. Unlike K-Means there is no requirement to pre-specify the number of clusters (though this is possible), rather sensitivity values are set which inform the process. Estimates of model parameters are initially randomly selected and then repeatedly tested and refined until the required quality is met \citep{hongbo2010data}.
